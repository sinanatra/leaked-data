{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Structured Data from search.libraryofleaks.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giacomonanni/Documents/Development/leaked-data/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"API\"\n",
    "BASE_URL = \"https://search.libraryofleaks.org/api/2\"\n",
    "HEADERS = {\"Authorization\": f\"ApiKey {API_KEY}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_collections():\n",
    "    \"\"\"Fetch all collections across paginated results.\"\"\"\n",
    "    url = f\"{BASE_URL}/collections\"\n",
    "    all_collections = []\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        all_collections.extend(data.get(\"results\", []))\n",
    "        url = data.get(\"next\", None)\n",
    "\n",
    "    return all_collections\n",
    "\n",
    "\n",
    "def fetch_schemas(collection_id):\n",
    "    \"\"\"Fetch schemas available in a specific collection.\"\"\"\n",
    "    url = f\"{BASE_URL}/collections/{collection_id}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response.raise_for_status()\n",
    "    collection_data = response.json()\n",
    "    schema_values = collection_data.get(\"statistics\", {}).get(\"schema\", {}).get(\"values\", {})\n",
    "    return list(schema_values.keys())\n",
    "\n",
    "def fetch_all_entities_by_schemas(collection_id, limit=100, max_offset=9900):\n",
    "    \"\"\"\n",
    "    Fetch all entities across all schemas for a collection.\n",
    "    \"\"\"\n",
    "    all_entities = []\n",
    "    schemas = fetch_schemas(collection_id)\n",
    "\n",
    "    for schema in schemas:\n",
    "        print(f\"Fetching entities for schema: {schema}\")\n",
    "        schema_entities = fetch_entities_with_offset(collection_id, schema, limit=limit, max_offset=max_offset)\n",
    "        all_entities.extend(schema_entities)\n",
    "\n",
    "    return all_entities\n",
    "\n",
    "def fetch_entities_with_offset(collection_id, schema, limit=100, max_offset=9900, retries=5, backoff_factor=2):\n",
    "    \"\"\"Fetch entities with retries and exponential backoff for rate-limiting (429) errors.\"\"\"\n",
    "    entities = []\n",
    "    for offset in range(0, max_offset, limit):\n",
    "        params = {\n",
    "            \"collection_id\": collection_id,\n",
    "            \"schema\": schema,\n",
    "            \"filter:schemata\": schema,\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "        }\n",
    "\n",
    "        url = f\"{BASE_URL}/entities\"\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                response = requests.get(url, headers=HEADERS, params=params)\n",
    "                response.raise_for_status() \n",
    "                data = response.json()\n",
    "                entities.extend(data.get(\"results\", []))\n",
    "                print(f\"Fetched {len(data.get('results', []))} entities from offset {offset}\")\n",
    "                break \n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if response.status_code == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", backoff_factor ** attempt))\n",
    "                    print(f\"429 Too Many Requests: Retrying in {retry_after} seconds...\")\n",
    "                    time.sleep(retry_after)\n",
    "                    attempt += 1\n",
    "                else:\n",
    "                    raise e  \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {backoff_factor ** attempt} seconds...\")\n",
    "                time.sleep(backoff_factor ** attempt)\n",
    "                attempt += 1\n",
    "        else:\n",
    "            print(f\"Failed to fetch data after {retries} retries for offset {offset}. Skipping this range.\")\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_entities_with_pagination(collection_id, schema, limit=100):\n",
    "    \"\"\"Fetch all entities for a specific collection and schema with pagination.\"\"\"\n",
    "    entities = []\n",
    "    params = {\"collection_id\": collection_id, \"schema\": schema, \"filter:schemata\": schema, \"limit\": limit}\n",
    "    url = f\"{BASE_URL}/entities\"\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url, headers=HEADERS, params=params if url == f\"{BASE_URL}/entities\" else None)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        entities.extend(data.get(\"results\", []))\n",
    "        url = data.get(\"next\")  \n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      "- 50: Aban Offshore\n",
      "- 13: Afghanistan Papiere\n",
      "- 51: Agencia Nacional de Hidrocarburos\n",
      "- 18: Airman Teixeira Leaks\n",
      "- 53: Alliance Coal\n",
      "- 35: Bahamas Registry\n",
      "- 1: BlueLeaks\n",
      "- 29: Chinga La Migra\n",
      "- 14: Constellis\n",
      "- 25: Cryptome Archive (2024)\n",
      "- 43: DJC Accountants\n",
      "- 15: Documents from US Espionage Den\n",
      "- 54: ENAMI EP\n",
      "- 39: Ethiopia Financial Intelligence Service\n",
      "- 19: FBIâ€™s Secret Rules\n",
      "- 56: Forest\n",
      "- 21: Fraternal Order of Police\n",
      "- 31: Fuck FBI Friday\n",
      "- 52: GorraLeaks\n",
      "- 55: Gulf Copper\n",
      "- 45: HBGary\n",
      "- 9: Hillary Clinton emails\n",
      "- 6: Hunter Biden emails\n",
      "- 57: INAFOR\n",
      "- 3: Israel Defense Forces (Anonymous For Justice)\n",
      "- 33: Israel Ministry of Justice\n",
      "- 4: Jones Day\n",
      "- 42: Kallias and Associates\n",
      "- 34: Kazakhstan Ministry of Energy\n",
      "- 38: LAPD Headshots\n",
      "- 40: LLC Capital\n",
      "- 11: Metropolitan Police Department D.C.\n",
      "- 16: MilicoLeaks\n",
      "- 47: Nauru Police Force\n",
      "- 41: Office of Industrial Economics, Thailand\n",
      "- 10: Paramilitary Election Interference\n",
      "- 23: Patron Papers\n",
      "- 32: Shooting Sheriffs Saturday\n",
      "- 22: Surveillance Device Catalogs\n",
      "- 24: Wikileaks Taskforce\n",
      "- 44: Worldwide Invest\n",
      "- 5: [WikiLeaks Archive] BND Inquiry\n",
      "- 28: [WikiLeaks Archive] Berat's Box\n",
      "- 27: [WikiLeaks Archive] Global Intelligence Files\n",
      "- 7: [WikiLeaks Archive] PlusD: Public Library of US Diplomacy\n",
      "- 26: [WikiLeaks Archive] Syria Files\n",
      "- 8: [WikiLeaks Archive] US Embassy Shopping List\n"
     ]
    }
   ],
   "source": [
    "collections = fetch_collections()\n",
    "sorted_collections = sorted(collections, key=lambda col: col['label'])\n",
    "\n",
    "print(\"Available collections:\")\n",
    "for col in sorted_collections:\n",
    "    print(f\"- {col['id']}: {col['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected:  33\n"
     ]
    }
   ],
   "source": [
    "collection_id = input(\"Enter the ID of the collection you want to explore: \")\n",
    "print(\"Selected: \", collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Page', 'HyperText', 'Image', 'Pages', 'Table', 'Person', 'PlainText', 'Workbook', 'Email', 'Folder', 'Document', 'Event', 'Package', 'Video', 'Audio']\n"
     ]
    }
   ],
   "source": [
    "schemas = fetch_schemas(collection_id)\n",
    "print(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching entities for schema: Page\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: HyperText\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Image\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Pages\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Table\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Person\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: PlainText\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Workbook\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Email\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Folder\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Document\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Event\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Package\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Video\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "Fetched 100 entities from offset 500\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "Fetched 100 entities from offset 600\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "Fetched 100 entities from offset 700\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "Fetched 100 entities from offset 800\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "429 Too Many Requests: Retrying in 16 seconds...\n",
      "Failed to fetch data after 5 retries for offset 900. Skipping this range.\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "429 Too Many Requests: Retrying in 16 seconds...\n",
      "Failed to fetch data after 5 retries for offset 1000. Skipping this range.\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "429 Too Many Requests: Retrying in 16 seconds...\n",
      "Failed to fetch data after 5 retries for offset 1100. Skipping this range.\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "429 Too Many Requests: Retrying in 16 seconds...\n",
      "Failed to fetch data after 5 retries for offset 1200. Skipping this range.\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "429 Too Many Requests: Retrying in 16 seconds...\n",
      "Failed to fetch data after 5 retries for offset 1300. Skipping this range.\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "429 Too Many Requests: Retrying in 16 seconds...\n",
      "Failed to fetch data after 5 retries for offset 1400. Skipping this range.\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "Fetched 100 entities from offset 1500\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "429 Too Many Requests: Retrying in 16 seconds...\n",
      "Failed to fetch data after 5 retries for offset 1600. Skipping this range.\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "429 Too Many Requests: Retrying in 2 seconds...\n",
      "429 Too Many Requests: Retrying in 4 seconds...\n",
      "429 Too Many Requests: Retrying in 8 seconds...\n",
      "429 Too Many Requests: Retrying in 16 seconds...\n",
      "Failed to fetch data after 5 retries for offset 1700. Skipping this range.\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetching entities for schema: Audio\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "Fetched 100 entities from offset 1100\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "429 Too Many Requests: Retrying in 1 seconds...\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    all_entities = fetch_all_entities_by_schemas(collection_id, max_offset=2000)\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"HTTPError occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities fetched: 29200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total entities fetched: {len(all_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(entities, remove_orphans=True):\n",
    "    \"\"\"Extract nodes, relationships, and bodyText from entities.\"\"\"\n",
    "    nodes = []\n",
    "    relations = []\n",
    "    body_text_data = []  \n",
    "    node_types = {}  \n",
    "\n",
    "    for entity in entities:\n",
    "        source = entity.get(\"id\", \"\")\n",
    "        schema = entity.get(\"schema\", \"\")\n",
    "        properties = entity.get(\"properties\", {})\n",
    "\n",
    "        \n",
    "        if source not in node_types:\n",
    "            node_data = {\n",
    "                \"id\": source,\n",
    "                \"schema\": schema,\n",
    "                \"type\": schema,  \n",
    "                \"created_at\": entity.get(\"created_at\", \"\"),\n",
    "                \"updated_at\": entity.get(\"updated_at\", \"\"),\n",
    "                \"fileSize\": properties.get(\"fileSize\", [None])[0],  \n",
    "                \"fileName\": properties.get(\"fileName\", [None])[0],  \n",
    "                \"mimeType\": properties.get(\"mimeType\", [None])[0],  \n",
    "            }\n",
    "            nodes.append(node_data)\n",
    "            node_types[source] = schema\n",
    "\n",
    "        \n",
    "        body_text = properties.get(\"bodyText\", [])\n",
    "        if isinstance(body_text, list) and body_text:\n",
    "            body_text_data.append({\n",
    "                \"id\": source,\n",
    "                \"bodyText\": \" \".join(body_text)\n",
    "            })\n",
    "\n",
    "        \n",
    "        documents = properties.get(\"document\", [])\n",
    "        if isinstance(documents, list):\n",
    "            for doc in documents:\n",
    "                if isinstance(doc, dict):  \n",
    "                    doc_id = doc.get(\"id\", \"\")\n",
    "                    doc_schema = doc.get(\"schema\", \"\")\n",
    "                    doc_properties = doc.get(\"properties\", {})\n",
    "\n",
    "                    \n",
    "                    if doc_id not in node_types:\n",
    "                        doc_node_data = {\n",
    "                            \"id\": doc_id,\n",
    "                            \"schema\": doc_schema,\n",
    "                            \"type\": doc_schema,  \n",
    "                            \"created_at\": doc.get(\"created_at\", \"\"),\n",
    "                            \"updated_at\": doc.get(\"updated_at\", \"\"),\n",
    "                            \"fileSize\": doc_properties.get(\"fileSize\", [None])[0],\n",
    "                            \"fileName\": doc_properties.get(\"fileName\", [None])[0],\n",
    "                            \"mimeType\": doc_properties.get(\"mimeType\", [None])[0],\n",
    "                        }\n",
    "                        nodes.append(doc_node_data)\n",
    "                        node_types[doc_id] = doc_schema\n",
    "\n",
    "                    \n",
    "                    nested_body_text = doc_properties.get(\"bodyText\", [])\n",
    "                    if isinstance(nested_body_text, list) and nested_body_text:\n",
    "                        body_text_data.append({\n",
    "                            \"id\": doc_id,\n",
    "                            \"bodyText\": \" \".join(nested_body_text)\n",
    "                        })\n",
    "\n",
    "                    \n",
    "                    for rel_key, rel_values in doc_properties.items():\n",
    "                        if isinstance(rel_values, list):\n",
    "                            for rel_value in rel_values:\n",
    "                                if isinstance(rel_value, dict):\n",
    "                                    target_id = rel_value.get(\"id\", \"\")\n",
    "                                    target_type = rel_key  \n",
    "                                    if target_id and target_id not in node_types:\n",
    "                                        \n",
    "                                        target_node_data = {\n",
    "                                            \"id\": target_id,\n",
    "                                            \"schema\": doc_schema,\n",
    "                                            \"type\": target_type,\n",
    "                                        }\n",
    "                                        nodes.append(target_node_data)\n",
    "                                        node_types[target_id] = target_type\n",
    "\n",
    "                                    \n",
    "                                    relations.append({\n",
    "                                        \"source\": doc_id,\n",
    "                                        \"target\": target_id,\n",
    "                                        \"relationship\": f\"has {rel_key}\",\n",
    "                                        \"source_type\": node_types.get(doc_id, \"\"),\n",
    "                                        \"target_type\": node_types.get(target_id, \"\"),\n",
    "                                    })\n",
    "                                else:\n",
    "                                    target_id = str(rel_value)\n",
    "                                    target_type = rel_key  \n",
    "                                    if target_id not in node_types:\n",
    "                                        \n",
    "                                        target_node_data = {\n",
    "                                            \"id\": target_id,\n",
    "                                            \"schema\": doc_schema,\n",
    "                                            \"type\": target_type,\n",
    "                                        }\n",
    "                                        nodes.append(target_node_data)\n",
    "                                        node_types[target_id] = target_type\n",
    "\n",
    "                                    \n",
    "                                    relations.append({\n",
    "                                        \"source\": doc_id,\n",
    "                                        \"target\": target_id,\n",
    "                                        \"relationship\": f\"has {rel_key}\",\n",
    "                                        \"source_type\": node_types.get(doc_id, \"\"),\n",
    "                                        \"target_type\": node_types.get(target_id, \"\"),\n",
    "                                    })\n",
    "\n",
    "    nodes_df = pd.DataFrame(nodes).fillna(\"\")\n",
    "    relations_df = pd.DataFrame(relations).fillna(\"\")\n",
    "    body_text_df = pd.DataFrame(body_text_data).fillna(\"\")\n",
    "\n",
    "    if remove_orphans:\n",
    "        \n",
    "        connected_nodes = set(relations_df[\"source\"]).union(set(relations_df[\"target\"]))\n",
    "        nodes_df = nodes_df[nodes_df[\"id\"].isin(connected_nodes)]\n",
    "\n",
    "    return nodes_df, relations_df, body_text_df\n",
    "\n",
    "def save_body_text(body_text_df, output_file=\"body_text.csv\"):\n",
    "    \"\"\"Save bodyText data to a separate file.\"\"\"\n",
    "    body_text_df.to_csv(output_file, index=False)\n",
    "    print(f\"BodyText data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, relations, body_text = extract_metadata(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(nodes, relations, collection_id):\n",
    "    \"\"\"Save nodes and relations to CSV for Gephi.\"\"\"\n",
    "    nodes = nodes.fillna(\"\")\n",
    "    relations = relations.fillna(\"\")\n",
    "\n",
    "    nodes_file = f\"collection_{collection_id}_nodes.csv\"\n",
    "    relations_file = f\"collection_{collection_id}_relations.csv\"\n",
    "    nodes.to_csv(nodes_file, index=False)\n",
    "    relations.to_csv(relations_file, index=False)\n",
    "    print(f\"Nodes saved to {nodes_file}\")\n",
    "    print(f\"Relations saved to {relations_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graphml(relations, nodes, output_file=\"network.graphml\"):\n",
    "    for col in [\"source\", \"target\", \"relationship\", \"source_type\", \"target_type\"]:\n",
    "        if col not in relations.columns:\n",
    "            relations[col] = \"\"\n",
    "\n",
    "    relations[\"source\"] = relations[\"source\"].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "    relations[\"target\"] = relations[\"target\"].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, node in nodes.iterrows():\n",
    "        node_id = node[\"id\"]\n",
    "  \n",
    "        node_attributes = {\n",
    "            \"schema\": node.get(\"schema\", \"\"),  \n",
    "            \"fileName\": node.get(\"fileName\", \"\"),\n",
    "            \"fileSize\": int(node[\"fileSize\"]) if str(node.get(\"fileSize\", \"\")).isdigit() else 0,\n",
    "            \"mimeType\": node.get(\"mimeType\", \"\"),\n",
    "            \"language\": node.get(\"language\", \"\"),\n",
    "            \"created_at\": node.get(\"created_at\", \"\"),  \n",
    "            \"updated_at\": node.get(\"updated_at\", \"\"),  \n",
    "            \"contentHash\": node.get(\"contentHash\", \"\")\n",
    "        }\n",
    "  \n",
    "        G.add_node(node_id, **node_attributes)\n",
    "\n",
    "    \n",
    "    for _, row in relations.iterrows():\n",
    "        source = row[\"source\"]\n",
    "        target = row[\"target\"]\n",
    "        relationship = row[\"relationship\"]\n",
    "        source_type = row[\"source_type\"]\n",
    "        target_type = row[\"target_type\"]\n",
    "\n",
    "        G.add_edge(source, target, relationship=relationship, source_type=source_type, target_type=target_type)\n",
    "\n",
    "    try:\n",
    "        nx.write_graphml(G, output_file)\n",
    "        print(f\"GraphML saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save GraphML: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphML saved to collection_33.graphml\n"
     ]
    }
   ],
   "source": [
    "save_graphml(relations, nodes, output_file=f\"data/collection_{collection_id}.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes saved to collection_33_nodes.csv\n",
      "Relations saved to collection_33_relations.csv\n"
     ]
    }
   ],
   "source": [
    "save_csv(nodes, relations, collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BodyText data saved to body_text_33.csv\n"
     ]
    }
   ],
   "source": [
    "save_body_text(body_text, output_file=f\"data/body_text_{collection_id}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adding specific filtering to ignore the processing information, language, etc that would link everything togetehr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(nodes_df, relations_df, output_file=\"graph.graphml\"):\n",
    "    \"\"\"Construct and save the graph in GraphML format.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, node in nodes_df.iterrows():\n",
    "        node_id = node[\"id\"]\n",
    "        node_attributes = node.drop(\"id\").to_dict()\n",
    "        G.add_node(node_id, **node_attributes)\n",
    "\n",
    "    for _, edge in relations_df.iterrows():\n",
    "        source = edge[\"source\"]\n",
    "        target = edge[\"target\"]\n",
    "        relationship = edge.get(\"relationship\", \"\")\n",
    "        G.add_edge(source, target, relationship=relationship)\n",
    "\n",
    "    nx.write_graphml(G, output_file)\n",
    "    print(f\"Graph saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to 33_cleaned_graph.graphml\n"
     ]
    }
   ],
   "source": [
    "unwanted_types = [\n",
    "    'detectedLanguage',\n",
    "    'mimeType',\n",
    "    # 'ancestors', \n",
    "    'processingAgent',\n",
    "    'processingStatus',\n",
    "    'processedAt'\n",
    "]\n",
    "\n",
    "nodes[\"fileSize\"] = pd.to_numeric(nodes[\"fileSize\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "nodes_cleaned = nodes[~nodes['type'].isin(unwanted_types)]\n",
    "nodes_cleaned.to_csv(f\"data/{collection_id}_cleaned_nodes.csv\", index=False)\n",
    "\n",
    "relations_cleaned = relations[~relations['target_type'].isin(unwanted_types)]\n",
    "relations_cleaned.to_csv(f\"data/{collection_id}_cleaned_relations.csv\", index=False)\n",
    "\n",
    "save_graph(nodes_cleaned, relations_cleaned, output_file=f\"data/{collection_id}_cleaned_graph.graphml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_body_text(body_text, output_file=f\"{collection_id}_body_text.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
