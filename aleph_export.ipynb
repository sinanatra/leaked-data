{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Structured Data from search.libraryofleaks.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"API\"\n",
    "BASE_URL = \"https://search.libraryofleaks.org/api/2\"\n",
    "HEADERS = {} # {\"Authorization\": f\"ApiKey {API_KEY}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_collections():\n",
    "    \"\"\"Fetch all collections across paginated results.\"\"\"\n",
    "    url = f\"{BASE_URL}/collections\"\n",
    "    all_collections = []\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        all_collections.extend(data.get(\"results\", []))\n",
    "        url = data.get(\"next\", None)\n",
    "\n",
    "    return all_collections\n",
    "\n",
    "\n",
    "def fetch_schemas(collection_id):\n",
    "    \"\"\"Fetch schemas available in a specific collection.\"\"\"\n",
    "    url = f\"{BASE_URL}/collections/{collection_id}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    collection_data = response.json()\n",
    "    schema_values = collection_data.get(\"statistics\", {}).get(\"schema\", {}).get(\"values\", {})\n",
    "    \n",
    "    \n",
    "    if isinstance(schema_values, dict):\n",
    "        return schema_values\n",
    "    else:\n",
    "        print(\"No schemas found or schema format unexpected.\")\n",
    "        return {}\n",
    "\n",
    "def fetch_all_entities_by_schemas(collection_id, limit=100, max_offset=None, offset_override=None):\n",
    "    \"\"\"\n",
    "    Fetch all entities across all schemas for a collection, with optional per-schema max offsets.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_entities = []\n",
    "    schemas = fetch_schemas(collection_id)\n",
    "\n",
    "    for schema, count in schemas.items():\n",
    "        print(f\"Fetching entities for schema: {schema} (Count: {count})\")\n",
    "        \n",
    "        schema_max_offset = offset_override.get(schema, max_offset) if offset_override else max_offset\n",
    "        effective_max_offset = min(schema_max_offset or count, count) \n",
    "        \n",
    "        schema_entities = fetch_entities_with_offset(\n",
    "            collection_id, schema, limit=limit, max_offset=effective_max_offset\n",
    "        )\n",
    "        all_entities.extend(schema_entities)\n",
    "\n",
    "    return all_entities\n",
    "\n",
    "\n",
    "\n",
    "def fetch_entities_with_offset(collection_id, schema, limit=100, max_offset=9900, retries=5, backoff_factor=2):\n",
    "    \"\"\"Fetch entities with retries and exponential backoff for rate-limiting (429) errors.\"\"\"\n",
    "    entities = []\n",
    "    for offset in range(0, max_offset, limit):\n",
    "        params = {\n",
    "            \"collection_id\": collection_id,\n",
    "            \"schema\": schema,\n",
    "            \"filter:schemata\": schema,\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "        }\n",
    "\n",
    "        url = f\"{BASE_URL}/entities\"\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                results = data.get(\"results\", [])\n",
    "                entities.extend(results)\n",
    "                print(f\"Fetched {len(results)} entities from offset {offset}\")\n",
    "                break\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if response.status_code == 429:  \n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", backoff_factor ** attempt))\n",
    "                    print(f\"429 Too Many Requests: Retrying in {retry_after} seconds...\")\n",
    "                    time.sleep(retry_after)\n",
    "                    attempt += 1\n",
    "                else:\n",
    "                    print(f\"HTTPError occurred: {e}\")\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {backoff_factor ** attempt} seconds...\")\n",
    "                time.sleep(backoff_factor ** attempt)\n",
    "                attempt += 1\n",
    "        else:\n",
    "            print(f\"Failed to fetch data after {retries} retries for offset {offset}. Skipping this range.\")\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def fetch_entities_with_pagination(collection_id, schema, limit=100):\n",
    "    \"\"\"Fetch all entities for a specific collection and schema with pagination.\"\"\"\n",
    "    entities = []\n",
    "    params = {\"collection_id\": collection_id, \"schema\": schema, \"filter:schemata\": schema, \"limit\": limit}\n",
    "    url = f\"{BASE_URL}/entities\"\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url, params=params if url == f\"{BASE_URL}/entities\" else None)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        entities.extend(data.get(\"results\", []))\n",
    "        url = data.get(\"next\")  \n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      "- 50: Aban Offshore\n",
      "- 13: Afghanistan Papiere\n",
      "- 51: Agencia Nacional de Hidrocarburos\n",
      "- 18: Airman Teixeira Leaks\n",
      "- 53: Alliance Coal\n",
      "- 35: Bahamas Registry\n",
      "- 1: BlueLeaks\n",
      "- 29: Chinga La Migra\n",
      "- 14: Constellis\n",
      "- 25: Cryptome Archive (2024)\n",
      "- 43: DJC Accountants\n",
      "- 15: Documents from US Espionage Den\n",
      "- 54: ENAMI EP\n",
      "- 39: Ethiopia Financial Intelligence Service\n",
      "- 19: FBIâ€™s Secret Rules\n",
      "- 56: Forest\n",
      "- 21: Fraternal Order of Police\n",
      "- 31: Fuck FBI Friday\n",
      "- 52: GorraLeaks\n",
      "- 55: Gulf Copper\n",
      "- 45: HBGary\n",
      "- 9: Hillary Clinton emails\n",
      "- 6: Hunter Biden emails\n",
      "- 57: INAFOR\n",
      "- 3: Israel Defense Forces (Anonymous For Justice)\n",
      "- 33: Israel Ministry of Justice\n",
      "- 4: Jones Day\n",
      "- 42: Kallias and Associates\n",
      "- 34: Kazakhstan Ministry of Energy\n",
      "- 38: LAPD Headshots\n",
      "- 40: LLC Capital\n",
      "- 11: Metropolitan Police Department D.C.\n",
      "- 16: MilicoLeaks\n",
      "- 47: Nauru Police Force\n",
      "- 41: Office of Industrial Economics, Thailand\n",
      "- 10: Paramilitary Election Interference\n",
      "- 23: Patron Papers\n",
      "- 32: Shooting Sheriffs Saturday\n",
      "- 22: Surveillance Device Catalogs\n",
      "- 24: Wikileaks Taskforce\n",
      "- 44: Worldwide Invest\n",
      "- 5: [WikiLeaks Archive] BND Inquiry\n",
      "- 28: [WikiLeaks Archive] Berat's Box\n",
      "- 27: [WikiLeaks Archive] Global Intelligence Files\n",
      "- 7: [WikiLeaks Archive] PlusD: Public Library of US Diplomacy\n",
      "- 26: [WikiLeaks Archive] Syria Files\n",
      "- 8: [WikiLeaks Archive] US Embassy Shopping List\n"
     ]
    }
   ],
   "source": [
    "collections = fetch_collections()\n",
    "sorted_collections = sorted(collections, key=lambda col: col['label'])\n",
    "\n",
    "print(\"Available collections:\")\n",
    "for col in sorted_collections:\n",
    "    print(f\"- {col['id']}: {col['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected:  10\n"
     ]
    }
   ],
   "source": [
    "collection_id = input(\"Enter the ID of the collection you want to explore: \")\n",
    "print(\"Selected: \", collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Image': 69341, 'Page': 41170, 'Video': 17180, 'Pages': 1129, 'HyperText': 485, 'Folder': 194, 'Audio': 117, 'Table': 75, 'PlainText': 50, 'Workbook': 15, 'Package': 8, 'Document': 4}\n"
     ]
    }
   ],
   "source": [
    "schemas = fetch_schemas(collection_id)\n",
    "print(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching entities for schema: Image (Count: 69341)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetched 100 entities from offset 2000\n",
      "Fetched 100 entities from offset 2100\n",
      "Fetched 100 entities from offset 2200\n",
      "Fetched 100 entities from offset 2300\n",
      "Fetched 100 entities from offset 2400\n",
      "Fetched 100 entities from offset 2500\n",
      "Fetched 100 entities from offset 2600\n",
      "Fetched 100 entities from offset 2700\n",
      "Fetched 100 entities from offset 2800\n",
      "Fetched 100 entities from offset 2900\n",
      "Fetched 100 entities from offset 3000\n",
      "Fetched 100 entities from offset 3100\n",
      "Fetched 100 entities from offset 3200\n",
      "Fetched 100 entities from offset 3300\n",
      "Fetched 100 entities from offset 3400\n",
      "Fetched 100 entities from offset 3500\n",
      "Fetched 100 entities from offset 3600\n",
      "Fetched 100 entities from offset 3700\n",
      "Fetched 100 entities from offset 3800\n",
      "Fetched 100 entities from offset 3900\n",
      "Fetched 100 entities from offset 4000\n",
      "Fetched 100 entities from offset 4100\n",
      "Fetched 100 entities from offset 4200\n",
      "Fetched 100 entities from offset 4300\n",
      "Fetched 100 entities from offset 4400\n",
      "Fetched 100 entities from offset 4500\n",
      "Fetched 100 entities from offset 4600\n",
      "Fetched 100 entities from offset 4700\n",
      "Fetched 100 entities from offset 4800\n",
      "Fetched 100 entities from offset 4900\n",
      "Fetched 100 entities from offset 5000\n",
      "Fetched 100 entities from offset 5100\n",
      "Fetched 100 entities from offset 5200\n",
      "Fetched 100 entities from offset 5300\n",
      "Fetched 100 entities from offset 5400\n",
      "Fetched 100 entities from offset 5500\n",
      "Fetched 100 entities from offset 5600\n",
      "Fetched 100 entities from offset 5700\n",
      "Fetched 100 entities from offset 5800\n",
      "Fetched 100 entities from offset 5900\n",
      "Fetched 100 entities from offset 6000\n",
      "Fetched 100 entities from offset 6100\n",
      "Fetched 100 entities from offset 6200\n",
      "Fetched 100 entities from offset 6300\n",
      "Fetched 100 entities from offset 6400\n",
      "Fetched 100 entities from offset 6500\n",
      "Fetched 100 entities from offset 6600\n",
      "Fetched 100 entities from offset 6700\n",
      "Fetched 100 entities from offset 6800\n",
      "Fetched 100 entities from offset 6900\n",
      "Fetched 100 entities from offset 7000\n",
      "Fetched 100 entities from offset 7100\n",
      "Fetched 100 entities from offset 7200\n",
      "Fetched 100 entities from offset 7300\n",
      "Fetched 100 entities from offset 7400\n",
      "Fetched 100 entities from offset 7500\n",
      "Fetched 100 entities from offset 7600\n",
      "Fetched 100 entities from offset 7700\n",
      "Fetched 100 entities from offset 7800\n",
      "Fetched 100 entities from offset 7900\n",
      "Fetched 100 entities from offset 8000\n",
      "Fetched 100 entities from offset 8100\n",
      "Fetched 100 entities from offset 8200\n",
      "Fetched 100 entities from offset 8300\n",
      "Fetched 100 entities from offset 8400\n",
      "Fetched 100 entities from offset 8500\n",
      "Fetched 100 entities from offset 8600\n",
      "Fetched 100 entities from offset 8700\n",
      "Fetched 100 entities from offset 8800\n",
      "Fetched 100 entities from offset 8900\n",
      "Fetched 100 entities from offset 9000\n",
      "Fetched 100 entities from offset 9100\n",
      "Fetched 100 entities from offset 9200\n",
      "Fetched 100 entities from offset 9300\n",
      "Fetched 100 entities from offset 9400\n",
      "Fetched 100 entities from offset 9500\n",
      "Fetched 100 entities from offset 9600\n",
      "Fetched 100 entities from offset 9700\n",
      "Fetched 100 entities from offset 9800\n",
      "Fetching entities for schema: Page (Count: 41170)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetched 100 entities from offset 2000\n",
      "Fetched 100 entities from offset 2100\n",
      "Fetched 100 entities from offset 2200\n",
      "Fetched 100 entities from offset 2300\n",
      "Fetched 100 entities from offset 2400\n",
      "Fetched 100 entities from offset 2500\n",
      "Fetched 100 entities from offset 2600\n",
      "Fetched 100 entities from offset 2700\n",
      "Fetched 100 entities from offset 2800\n",
      "Fetched 100 entities from offset 2900\n",
      "Fetched 100 entities from offset 3000\n",
      "Fetched 100 entities from offset 3100\n",
      "Fetched 100 entities from offset 3200\n",
      "Fetched 100 entities from offset 3300\n",
      "Fetched 100 entities from offset 3400\n",
      "Fetched 100 entities from offset 3500\n",
      "Fetched 100 entities from offset 3600\n",
      "Fetched 100 entities from offset 3700\n",
      "Fetched 100 entities from offset 3800\n",
      "Fetched 100 entities from offset 3900\n",
      "Fetched 100 entities from offset 4000\n",
      "Fetched 100 entities from offset 4100\n",
      "Fetched 100 entities from offset 4200\n",
      "Fetched 100 entities from offset 4300\n",
      "Fetched 100 entities from offset 4400\n",
      "Fetched 100 entities from offset 4500\n",
      "Fetched 100 entities from offset 4600\n",
      "Fetched 100 entities from offset 4700\n",
      "Fetched 100 entities from offset 4800\n",
      "Fetched 100 entities from offset 4900\n",
      "Fetched 100 entities from offset 5000\n",
      "Fetched 100 entities from offset 5100\n",
      "Fetched 100 entities from offset 5200\n",
      "Fetched 100 entities from offset 5300\n",
      "Fetched 100 entities from offset 5400\n",
      "Fetched 100 entities from offset 5500\n",
      "Fetched 100 entities from offset 5600\n",
      "Fetched 100 entities from offset 5700\n",
      "Fetched 100 entities from offset 5800\n",
      "Fetched 100 entities from offset 5900\n",
      "Fetched 100 entities from offset 6000\n",
      "Fetched 100 entities from offset 6100\n",
      "Fetched 100 entities from offset 6200\n",
      "Fetched 100 entities from offset 6300\n",
      "Fetched 100 entities from offset 6400\n",
      "Fetched 100 entities from offset 6500\n",
      "Fetched 100 entities from offset 6600\n",
      "Fetched 100 entities from offset 6700\n",
      "Fetched 100 entities from offset 6800\n",
      "Fetched 100 entities from offset 6900\n",
      "Fetched 100 entities from offset 7000\n",
      "Fetched 100 entities from offset 7100\n",
      "Fetched 100 entities from offset 7200\n",
      "Fetched 100 entities from offset 7300\n",
      "Fetched 100 entities from offset 7400\n",
      "Fetched 100 entities from offset 7500\n",
      "Fetched 100 entities from offset 7600\n",
      "Fetched 100 entities from offset 7700\n",
      "Fetched 100 entities from offset 7800\n",
      "Fetched 100 entities from offset 7900\n",
      "Fetched 100 entities from offset 8000\n",
      "Fetched 100 entities from offset 8100\n",
      "Fetched 100 entities from offset 8200\n",
      "Fetched 100 entities from offset 8300\n",
      "Fetched 100 entities from offset 8400\n",
      "Fetched 100 entities from offset 8500\n",
      "Fetched 100 entities from offset 8600\n",
      "Fetched 100 entities from offset 8700\n",
      "Fetched 100 entities from offset 8800\n",
      "Fetched 100 entities from offset 8900\n",
      "Fetched 100 entities from offset 9000\n",
      "Fetched 100 entities from offset 9100\n",
      "Fetched 100 entities from offset 9200\n",
      "Fetched 100 entities from offset 9300\n",
      "Fetched 100 entities from offset 9400\n",
      "Fetched 100 entities from offset 9500\n",
      "Fetched 100 entities from offset 9600\n",
      "Fetched 100 entities from offset 9700\n",
      "Fetched 100 entities from offset 9800\n",
      "Fetching entities for schema: Video (Count: 17180)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetched 100 entities from offset 2000\n",
      "Fetched 100 entities from offset 2100\n",
      "Fetched 100 entities from offset 2200\n",
      "Fetched 100 entities from offset 2300\n",
      "Fetched 100 entities from offset 2400\n",
      "Fetched 100 entities from offset 2500\n",
      "Fetched 100 entities from offset 2600\n",
      "Fetched 100 entities from offset 2700\n",
      "Fetched 100 entities from offset 2800\n",
      "Fetched 100 entities from offset 2900\n",
      "Fetched 100 entities from offset 3000\n",
      "Fetched 100 entities from offset 3100\n",
      "Fetched 100 entities from offset 3200\n",
      "Fetched 100 entities from offset 3300\n",
      "Fetched 100 entities from offset 3400\n",
      "Fetched 100 entities from offset 3500\n",
      "Fetched 100 entities from offset 3600\n",
      "Fetched 100 entities from offset 3700\n",
      "Fetched 100 entities from offset 3800\n",
      "Fetched 100 entities from offset 3900\n",
      "Fetched 100 entities from offset 4000\n",
      "Fetched 100 entities from offset 4100\n",
      "Fetched 100 entities from offset 4200\n",
      "Fetched 100 entities from offset 4300\n",
      "Fetched 100 entities from offset 4400\n",
      "Fetched 100 entities from offset 4500\n",
      "Fetched 100 entities from offset 4600\n",
      "Fetched 100 entities from offset 4700\n",
      "Fetched 100 entities from offset 4800\n",
      "Fetched 100 entities from offset 4900\n",
      "Fetched 100 entities from offset 5000\n",
      "Fetched 100 entities from offset 5100\n",
      "Fetched 100 entities from offset 5200\n",
      "Fetched 100 entities from offset 5300\n",
      "Fetched 100 entities from offset 5400\n",
      "Fetched 100 entities from offset 5500\n",
      "Fetched 100 entities from offset 5600\n",
      "Fetched 100 entities from offset 5700\n",
      "Fetched 100 entities from offset 5800\n",
      "Fetched 100 entities from offset 5900\n",
      "Fetched 100 entities from offset 6000\n",
      "Fetched 100 entities from offset 6100\n",
      "Fetched 100 entities from offset 6200\n",
      "Fetched 100 entities from offset 6300\n",
      "Fetched 100 entities from offset 6400\n",
      "Fetched 100 entities from offset 6500\n",
      "Fetched 100 entities from offset 6600\n",
      "Fetched 100 entities from offset 6700\n",
      "Fetched 100 entities from offset 6800\n",
      "Fetched 100 entities from offset 6900\n",
      "Fetched 100 entities from offset 7000\n",
      "Fetched 100 entities from offset 7100\n",
      "Fetched 100 entities from offset 7200\n",
      "Fetched 100 entities from offset 7300\n",
      "Fetched 100 entities from offset 7400\n",
      "Fetched 100 entities from offset 7500\n",
      "Fetched 100 entities from offset 7600\n",
      "Fetched 100 entities from offset 7700\n",
      "Fetched 100 entities from offset 7800\n",
      "Fetched 100 entities from offset 7900\n",
      "Fetched 100 entities from offset 8000\n",
      "Fetched 100 entities from offset 8100\n",
      "Fetched 100 entities from offset 8200\n",
      "Fetched 100 entities from offset 8300\n",
      "Fetched 100 entities from offset 8400\n",
      "Fetched 100 entities from offset 8500\n",
      "Fetched 100 entities from offset 8600\n",
      "Fetched 100 entities from offset 8700\n",
      "Fetched 100 entities from offset 8800\n",
      "Fetched 100 entities from offset 8900\n",
      "Fetched 100 entities from offset 9000\n",
      "Fetched 100 entities from offset 9100\n",
      "Fetched 100 entities from offset 9200\n",
      "Fetched 100 entities from offset 9300\n",
      "Fetched 100 entities from offset 9400\n",
      "Fetched 100 entities from offset 9500\n",
      "Fetched 100 entities from offset 9600\n",
      "Fetched 100 entities from offset 9700\n",
      "Fetched 100 entities from offset 9800\n",
      "Fetching entities for schema: Pages (Count: 1129)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetching entities for schema: HyperText (Count: 485)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetching entities for schema: Folder (Count: 194)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetching entities for schema: Audio (Count: 117)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetching entities for schema: Table (Count: 75)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetching entities for schema: PlainText (Count: 50)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetching entities for schema: Workbook (Count: 15)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetching entities for schema: Package (Count: 8)\n",
      "Fetched 100 entities from offset 0\n",
      "Fetching entities for schema: Document (Count: 4)\n",
      "Fetched 100 entities from offset 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    all_entities = fetch_all_entities_by_schemas(collection_id, max_offset=9900)\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"HTTPError occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities fetched: 32300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total entities fetched: {len(all_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(entities, remove_orphans=True):\n",
    "    \"\"\"Extract nodes, relationships, and bodyText from entities.\"\"\"\n",
    "    nodes = []\n",
    "    relations = []\n",
    "    body_text_data = []  \n",
    "    node_types = {}  \n",
    "\n",
    "    for entity in entities:\n",
    "        source = entity.get(\"id\", \"\")\n",
    "        schema = entity.get(\"schema\", \"\")\n",
    "        properties = entity.get(\"properties\", {})\n",
    "\n",
    "        \n",
    "        if source not in node_types:\n",
    "            node_data = {\n",
    "                \"id\": source,\n",
    "                \"schema\": schema,\n",
    "                \"type\": schema,  \n",
    "                \"created_at\": entity.get(\"created_at\", \"\"),\n",
    "                \"updated_at\": entity.get(\"updated_at\", \"\"),\n",
    "                \"fileSize\": properties.get(\"fileSize\", [None])[0],  \n",
    "                \"fileName\": properties.get(\"fileName\", [None])[0],  \n",
    "                \"mimeType\": properties.get(\"mimeType\", [None])[0],  \n",
    "            }\n",
    "            nodes.append(node_data)\n",
    "            node_types[source] = schema\n",
    "\n",
    "        \n",
    "        body_text = properties.get(\"bodyText\", [])\n",
    "        if isinstance(body_text, list) and body_text:\n",
    "            body_text_data.append({\n",
    "                \"id\": source,\n",
    "                \"bodyText\": \" \".join(body_text)\n",
    "            })\n",
    "\n",
    "        \n",
    "        documents = properties.get(\"document\", [])\n",
    "        if isinstance(documents, list):\n",
    "            for doc in documents:\n",
    "                if isinstance(doc, dict):  \n",
    "                    doc_id = doc.get(\"id\", \"\")\n",
    "                    doc_schema = doc.get(\"schema\", \"\")\n",
    "                    doc_properties = doc.get(\"properties\", {})\n",
    "\n",
    "                    \n",
    "                    if doc_id not in node_types:\n",
    "                        doc_node_data = {\n",
    "                            \"id\": doc_id,\n",
    "                            \"schema\": doc_schema,\n",
    "                            \"type\": doc_schema,  \n",
    "                            \"created_at\": doc.get(\"created_at\", \"\"),\n",
    "                            \"updated_at\": doc.get(\"updated_at\", \"\"),\n",
    "                            \"fileSize\": doc_properties.get(\"fileSize\", [None])[0],\n",
    "                            \"fileName\": doc_properties.get(\"fileName\", [None])[0],\n",
    "                            \"mimeType\": doc_properties.get(\"mimeType\", [None])[0],\n",
    "                        }\n",
    "                        nodes.append(doc_node_data)\n",
    "                        node_types[doc_id] = doc_schema\n",
    "\n",
    "                    \n",
    "                    nested_body_text = doc_properties.get(\"bodyText\", [])\n",
    "                    if isinstance(nested_body_text, list) and nested_body_text:\n",
    "                        body_text_data.append({\n",
    "                            \"id\": doc_id,\n",
    "                            \"bodyText\": \" \".join(nested_body_text)\n",
    "                        })\n",
    "\n",
    "                    \n",
    "                    for rel_key, rel_values in doc_properties.items():\n",
    "                        if isinstance(rel_values, list):\n",
    "                            for rel_value in rel_values:\n",
    "                                if isinstance(rel_value, dict):\n",
    "                                    target_id = rel_value.get(\"id\", \"\")\n",
    "                                    target_type = rel_key  \n",
    "                                    if target_id and target_id not in node_types:\n",
    "                                        \n",
    "                                        target_node_data = {\n",
    "                                            \"id\": target_id,\n",
    "                                            \"schema\": doc_schema,\n",
    "                                            \"type\": target_type,\n",
    "                                        }\n",
    "                                        nodes.append(target_node_data)\n",
    "                                        node_types[target_id] = target_type\n",
    "\n",
    "                                    \n",
    "                                    relations.append({\n",
    "                                        \"source\": doc_id,\n",
    "                                        \"target\": target_id,\n",
    "                                        \"relationship\": f\"has {rel_key}\",\n",
    "                                        \"source_type\": node_types.get(doc_id, \"\"),\n",
    "                                        \"target_type\": node_types.get(target_id, \"\"),\n",
    "                                    })\n",
    "                                else:\n",
    "                                    target_id = str(rel_value)\n",
    "                                    target_type = rel_key  \n",
    "                                    if target_id not in node_types:\n",
    "                                        \n",
    "                                        target_node_data = {\n",
    "                                            \"id\": target_id,\n",
    "                                            \"schema\": doc_schema,\n",
    "                                            \"type\": target_type,\n",
    "                                        }\n",
    "                                        nodes.append(target_node_data)\n",
    "                                        node_types[target_id] = target_type\n",
    "\n",
    "                                    \n",
    "                                    relations.append({\n",
    "                                        \"source\": doc_id,\n",
    "                                        \"target\": target_id,\n",
    "                                        \"relationship\": f\"has {rel_key}\",\n",
    "                                        \"source_type\": node_types.get(doc_id, \"\"),\n",
    "                                        \"target_type\": node_types.get(target_id, \"\"),\n",
    "                                    })\n",
    "\n",
    "    nodes_df = pd.DataFrame(nodes).fillna(\"\")\n",
    "    relations_df = pd.DataFrame(relations).fillna(\"\")\n",
    "    body_text_df = pd.DataFrame(body_text_data).fillna(\"\")\n",
    "\n",
    "    if remove_orphans:\n",
    "        \n",
    "        connected_nodes = set(relations_df[\"source\"]).union(set(relations_df[\"target\"]))\n",
    "        nodes_df = nodes_df[nodes_df[\"id\"].isin(connected_nodes)]\n",
    "\n",
    "    return nodes_df, relations_df, body_text_df\n",
    "\n",
    "def save_body_text(body_text_df, output_file=\"body_text.csv\"):\n",
    "    \"\"\"Save bodyText data to a separate file.\"\"\"\n",
    "    body_text_df.to_csv(output_file, index=False)\n",
    "    print(f\"BodyText data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, relations, body_text = extract_metadata(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(nodes, relations, collection_id):\n",
    "    \"\"\"Save nodes and relations to CSV for Gephi.\"\"\"\n",
    "    nodes = nodes.fillna(\"\")\n",
    "    relations = relations.fillna(\"\")\n",
    "\n",
    "    nodes_file = f\"data/collection_{collection_id}_nodes.csv\"\n",
    "    relations_file = f\"data/collection_{collection_id}_relations.csv\"\n",
    "    nodes.to_csv(nodes_file, index=False)\n",
    "    relations.to_csv(relations_file, index=False)\n",
    "    print(f\"Nodes saved to {nodes_file}\")\n",
    "    print(f\"Relations saved to {relations_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def save_graph(nodes_df, relations_df, output_file=\"graph.graphml\"):\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, node in nodes_df.iterrows():\n",
    "        node_id = node[\"id\"]\n",
    "        node_attributes = node.drop(\"id\").to_dict()\n",
    "        G.add_node(node_id, **node_attributes)\n",
    "\n",
    "    for _, edge in relations_df.iterrows():\n",
    "        source = edge[\"source\"]\n",
    "        target = edge[\"target\"]\n",
    "        edge_attributes = edge.drop([\"source\", \"target\"]).to_dict()\n",
    "        G.add_edge(source, target, **edge_attributes)\n",
    "\n",
    "    nx.write_graphml(G, output_file)\n",
    "    print(f\"Graph saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Save all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to data/collection_10.graphml\n"
     ]
    }
   ],
   "source": [
    "nodes = nodes.fillna(\"\")\n",
    "relations = relations.fillna(\"\")\n",
    "\n",
    "save_graph(nodes, relations, output_file=f\"data/collection_{collection_id}.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes saved to collection_10_nodes.csv\n",
      "Relations saved to collection_10_relations.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "save_csv(nodes, relations, collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adding specific filtering to ignore the processing information, language, etc that would link everything togetehr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to data/9_cleaned_graph.graphml\n"
     ]
    }
   ],
   "source": [
    "unwanted_types = [\n",
    "    'detectedLanguage',\n",
    "    'mimeType',\n",
    "    # 'ancestors', \n",
    "    'processingAgent',\n",
    "    'processingStatus',\n",
    "    'processedAt'\n",
    "]\n",
    "\n",
    "nodes[\"fileSize\"] = pd.to_numeric(nodes[\"fileSize\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "nodes_cleaned = nodes[~nodes['type'].isin(unwanted_types)]\n",
    "nodes_cleaned.to_csv(f\"data/{collection_id}_cleaned_nodes.csv\", index=False)\n",
    "\n",
    "relations_cleaned = relations[~relations['target_type'].isin(unwanted_types)]\n",
    "relations_cleaned.to_csv(f\"data/{collection_id}_cleaned_relations.csv\", index=False)\n",
    "\n",
    "save_graph(nodes_cleaned, relations_cleaned, output_file=f\"data/{collection_id}_cleaned_graph.graphml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BodyText data saved to data/9_body_text.csv\n"
     ]
    }
   ],
   "source": [
    "save_body_text(body_text, output_file=f\"data/{collection_id}_body_text.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
