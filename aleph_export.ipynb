{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Structured Data from search.libraryofleaks.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"API\"\n",
    "BASE_URL = \"https://search.libraryofleaks.org/api/2\"\n",
    "HEADERS = {} # {\"Authorization\": f\"ApiKey {API_KEY}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_collections():\n",
    "    \"\"\"Fetch all collections across paginated results.\"\"\"\n",
    "    url = f\"{BASE_URL}/collections\"\n",
    "    all_collections = []\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        all_collections.extend(data.get(\"results\", []))\n",
    "        url = data.get(\"next\", None)\n",
    "\n",
    "    return all_collections\n",
    "\n",
    "\n",
    "def fetch_schemas(collection_id):\n",
    "    \"\"\"Fetch schemas available in a specific collection.\"\"\"\n",
    "    url = f\"{BASE_URL}/collections/{collection_id}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    collection_data = response.json()\n",
    "    schema_values = collection_data.get(\"statistics\", {}).get(\"schema\", {}).get(\"values\", {})\n",
    "    return list(schema_values.keys())\n",
    "\n",
    "def fetch_all_entities_by_schemas(collection_id, limit=100, max_offset=9900):\n",
    "    \"\"\"\n",
    "    Fetch all entities across all schemas for a collection.\n",
    "    \"\"\"\n",
    "    all_entities = []\n",
    "    schemas = fetch_schemas(collection_id)\n",
    "\n",
    "    for schema in schemas:\n",
    "        print(f\"Fetching entities for schema: {schema}\")\n",
    "        schema_entities = fetch_entities_with_offset(collection_id, schema, limit=limit, max_offset=max_offset)\n",
    "        all_entities.extend(schema_entities)\n",
    "\n",
    "    return all_entities\n",
    "\n",
    "def fetch_entities_with_offset(collection_id, schema, limit=100, max_offset=9900, retries=5, backoff_factor=2):\n",
    "    \"\"\"Fetch entities with retries and exponential backoff for rate-limiting (429) errors.\"\"\"\n",
    "    entities = []\n",
    "    for offset in range(0, max_offset, limit):\n",
    "        params = {\n",
    "            \"collection_id\": collection_id,\n",
    "            \"schema\": schema,\n",
    "            \"filter:schemata\": schema,\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "        }\n",
    "\n",
    "        url = f\"{BASE_URL}/entities\"\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status() \n",
    "                data = response.json()\n",
    "                entities.extend(data.get(\"results\", []))\n",
    "                print(f\"Fetched {len(data.get('results', []))} entities from offset {offset}\")\n",
    "                break \n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if response.status_code == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", backoff_factor ** attempt))\n",
    "                    print(f\"429 Too Many Requests: Retrying in {retry_after} seconds...\")\n",
    "                    time.sleep(retry_after)\n",
    "                    attempt += 1\n",
    "                else:\n",
    "                    raise e  \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in {backoff_factor ** attempt} seconds...\")\n",
    "                time.sleep(backoff_factor ** attempt)\n",
    "                attempt += 1\n",
    "        else:\n",
    "            print(f\"Failed to fetch data after {retries} retries for offset {offset}. Skipping this range.\")\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_entities_with_pagination(collection_id, schema, limit=100):\n",
    "    \"\"\"Fetch all entities for a specific collection and schema with pagination.\"\"\"\n",
    "    entities = []\n",
    "    params = {\"collection_id\": collection_id, \"schema\": schema, \"filter:schemata\": schema, \"limit\": limit}\n",
    "    url = f\"{BASE_URL}/entities\"\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url, params=params if url == f\"{BASE_URL}/entities\" else None)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        entities.extend(data.get(\"results\", []))\n",
    "        url = data.get(\"next\")  \n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      "- 50: Aban Offshore\n",
      "- 13: Afghanistan Papiere\n",
      "- 51: Agencia Nacional de Hidrocarburos\n",
      "- 18: Airman Teixeira Leaks\n",
      "- 53: Alliance Coal\n",
      "- 35: Bahamas Registry\n",
      "- 1: BlueLeaks\n",
      "- 29: Chinga La Migra\n",
      "- 14: Constellis\n",
      "- 25: Cryptome Archive (2024)\n",
      "- 43: DJC Accountants\n",
      "- 15: Documents from US Espionage Den\n",
      "- 54: ENAMI EP\n",
      "- 39: Ethiopia Financial Intelligence Service\n",
      "- 19: FBIâ€™s Secret Rules\n",
      "- 56: Forest\n",
      "- 21: Fraternal Order of Police\n",
      "- 31: Fuck FBI Friday\n",
      "- 52: GorraLeaks\n",
      "- 55: Gulf Copper\n",
      "- 45: HBGary\n",
      "- 9: Hillary Clinton emails\n",
      "- 6: Hunter Biden emails\n",
      "- 57: INAFOR\n",
      "- 3: Israel Defense Forces (Anonymous For Justice)\n",
      "- 33: Israel Ministry of Justice\n",
      "- 4: Jones Day\n",
      "- 42: Kallias and Associates\n",
      "- 34: Kazakhstan Ministry of Energy\n",
      "- 38: LAPD Headshots\n",
      "- 40: LLC Capital\n",
      "- 11: Metropolitan Police Department D.C.\n",
      "- 16: MilicoLeaks\n",
      "- 47: Nauru Police Force\n",
      "- 41: Office of Industrial Economics, Thailand\n",
      "- 10: Paramilitary Election Interference\n",
      "- 23: Patron Papers\n",
      "- 32: Shooting Sheriffs Saturday\n",
      "- 22: Surveillance Device Catalogs\n",
      "- 24: Wikileaks Taskforce\n",
      "- 44: Worldwide Invest\n",
      "- 5: [WikiLeaks Archive] BND Inquiry\n",
      "- 28: [WikiLeaks Archive] Berat's Box\n",
      "- 27: [WikiLeaks Archive] Global Intelligence Files\n",
      "- 7: [WikiLeaks Archive] PlusD: Public Library of US Diplomacy\n",
      "- 26: [WikiLeaks Archive] Syria Files\n",
      "- 8: [WikiLeaks Archive] US Embassy Shopping List\n"
     ]
    }
   ],
   "source": [
    "collections = fetch_collections()\n",
    "sorted_collections = sorted(collections, key=lambda col: col['label'])\n",
    "\n",
    "print(\"Available collections:\")\n",
    "for col in sorted_collections:\n",
    "    print(f\"- {col['id']}: {col['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected:  33\n"
     ]
    }
   ],
   "source": [
    "collection_id = input(\"Enter the ID of the collection you want to explore: \")\n",
    "print(\"Selected: \", collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://search.libraryofleaks.org/api/2/collections/33",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[299], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m schemas \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_schemas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(schemas)\n",
      "Cell \u001b[0;32mIn[297], line 20\u001b[0m, in \u001b[0;36mfetch_schemas\u001b[0;34m(collection_id)\u001b[0m\n\u001b[1;32m     18\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/collections/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mHEADERS)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m collection_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     22\u001b[0m schema_values \u001b[38;5;241m=\u001b[39m collection_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatistics\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "File \u001b[0;32m~/Documents/Development/leaked-data/.venv/lib/python3.9/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://search.libraryofleaks.org/api/2/collections/33"
     ]
    }
   ],
   "source": [
    "schemas = fetch_schemas(collection_id)\n",
    "print(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching entities for schema: Page\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetched 100 entities from offset 2000\n",
      "Fetched 100 entities from offset 2100\n",
      "Fetched 100 entities from offset 2200\n",
      "Fetched 100 entities from offset 2300\n",
      "Fetched 100 entities from offset 2400\n",
      "Fetched 100 entities from offset 2500\n",
      "Fetched 100 entities from offset 2600\n",
      "Fetched 100 entities from offset 2700\n",
      "Fetched 100 entities from offset 2800\n",
      "Fetched 100 entities from offset 2900\n",
      "Fetched 100 entities from offset 3000\n",
      "Fetched 100 entities from offset 3100\n",
      "Fetched 100 entities from offset 3200\n",
      "Fetched 100 entities from offset 3300\n",
      "Fetched 100 entities from offset 3400\n",
      "Fetched 100 entities from offset 3500\n",
      "Fetched 100 entities from offset 3600\n",
      "Fetched 100 entities from offset 3700\n",
      "Fetched 100 entities from offset 3800\n",
      "Fetched 100 entities from offset 3900\n",
      "Fetched 100 entities from offset 4000\n",
      "Fetched 100 entities from offset 4100\n",
      "Fetched 100 entities from offset 4200\n",
      "Fetched 100 entities from offset 4300\n",
      "Fetched 100 entities from offset 4400\n",
      "Fetched 100 entities from offset 4500\n",
      "Fetched 100 entities from offset 4600\n",
      "Fetched 100 entities from offset 4700\n",
      "Fetched 100 entities from offset 4800\n",
      "Fetched 100 entities from offset 4900\n",
      "Fetching entities for schema: Pages\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetched 100 entities from offset 2000\n",
      "Fetched 100 entities from offset 2100\n",
      "Fetched 100 entities from offset 2200\n",
      "Fetched 100 entities from offset 2300\n",
      "Fetched 100 entities from offset 2400\n",
      "Fetched 100 entities from offset 2500\n",
      "Fetched 100 entities from offset 2600\n",
      "Fetched 100 entities from offset 2700\n",
      "Fetched 100 entities from offset 2800\n",
      "Fetched 100 entities from offset 2900\n",
      "Fetched 100 entities from offset 3000\n",
      "Fetched 100 entities from offset 3100\n",
      "Fetched 100 entities from offset 3200\n",
      "Fetched 100 entities from offset 3300\n",
      "Fetched 100 entities from offset 3400\n",
      "Fetched 100 entities from offset 3500\n",
      "Fetched 100 entities from offset 3600\n",
      "Fetched 100 entities from offset 3700\n",
      "Fetched 100 entities from offset 3800\n",
      "Fetched 100 entities from offset 3900\n",
      "Fetched 100 entities from offset 4000\n",
      "Fetched 100 entities from offset 4100\n",
      "Fetched 100 entities from offset 4200\n",
      "Fetched 100 entities from offset 4300\n",
      "Fetched 100 entities from offset 4400\n",
      "Fetched 100 entities from offset 4500\n",
      "Fetched 100 entities from offset 4600\n",
      "Fetched 100 entities from offset 4700\n",
      "Fetched 100 entities from offset 4800\n",
      "Fetched 100 entities from offset 4900\n",
      "Fetching entities for schema: Folder\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetched 100 entities from offset 1000\n",
      "Fetched 100 entities from offset 1100\n",
      "Fetched 100 entities from offset 1200\n",
      "Fetched 100 entities from offset 1300\n",
      "Fetched 100 entities from offset 1400\n",
      "Fetched 100 entities from offset 1500\n",
      "Fetched 100 entities from offset 1600\n",
      "Fetched 100 entities from offset 1700\n",
      "Fetched 100 entities from offset 1800\n",
      "Fetched 100 entities from offset 1900\n",
      "Fetched 100 entities from offset 2000\n",
      "Fetched 100 entities from offset 2100\n",
      "Fetched 100 entities from offset 2200\n",
      "Fetched 100 entities from offset 2300\n",
      "Fetched 100 entities from offset 2400\n",
      "Fetched 100 entities from offset 2500\n",
      "Fetched 100 entities from offset 2600\n",
      "Fetched 100 entities from offset 2700\n",
      "Fetched 100 entities from offset 2800\n",
      "Fetched 100 entities from offset 2900\n",
      "Fetched 100 entities from offset 3000\n",
      "Fetched 100 entities from offset 3100\n",
      "Fetched 100 entities from offset 3200\n",
      "Fetched 100 entities from offset 3300\n",
      "Fetched 100 entities from offset 3400\n",
      "Fetched 100 entities from offset 3500\n",
      "Fetched 100 entities from offset 3600\n",
      "Fetched 100 entities from offset 3700\n",
      "Fetched 100 entities from offset 3800\n",
      "Fetched 100 entities from offset 3900\n",
      "Fetched 100 entities from offset 4000\n",
      "Fetched 100 entities from offset 4100\n",
      "Fetched 100 entities from offset 4200\n",
      "Fetched 100 entities from offset 4300\n",
      "Fetched 100 entities from offset 4400\n",
      "Fetched 100 entities from offset 4500\n",
      "Fetched 100 entities from offset 4600\n",
      "Fetched 100 entities from offset 4700\n",
      "Fetched 100 entities from offset 4800\n",
      "Fetched 100 entities from offset 4900\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    all_entities = fetch_all_entities_by_schemas(collection_id, max_offset=5000)\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"HTTPError occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities fetched: 15000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total entities fetched: {len(all_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(entities, remove_orphans=True):\n",
    "    \"\"\"Extract nodes, relationships, and bodyText from entities.\"\"\"\n",
    "    nodes = []\n",
    "    relations = []\n",
    "    body_text_data = []  \n",
    "    node_types = {}  \n",
    "\n",
    "    for entity in entities:\n",
    "        source = entity.get(\"id\", \"\")\n",
    "        schema = entity.get(\"schema\", \"\")\n",
    "        properties = entity.get(\"properties\", {})\n",
    "\n",
    "        \n",
    "        if source not in node_types:\n",
    "            node_data = {\n",
    "                \"id\": source,\n",
    "                \"schema\": schema,\n",
    "                \"type\": schema,  \n",
    "                \"created_at\": entity.get(\"created_at\", \"\"),\n",
    "                \"updated_at\": entity.get(\"updated_at\", \"\"),\n",
    "                \"fileSize\": properties.get(\"fileSize\", [None])[0],  \n",
    "                \"fileName\": properties.get(\"fileName\", [None])[0],  \n",
    "                \"mimeType\": properties.get(\"mimeType\", [None])[0],  \n",
    "            }\n",
    "            nodes.append(node_data)\n",
    "            node_types[source] = schema\n",
    "\n",
    "        \n",
    "        body_text = properties.get(\"bodyText\", [])\n",
    "        if isinstance(body_text, list) and body_text:\n",
    "            body_text_data.append({\n",
    "                \"id\": source,\n",
    "                \"bodyText\": \" \".join(body_text)\n",
    "            })\n",
    "\n",
    "        \n",
    "        documents = properties.get(\"document\", [])\n",
    "        if isinstance(documents, list):\n",
    "            for doc in documents:\n",
    "                if isinstance(doc, dict):  \n",
    "                    doc_id = doc.get(\"id\", \"\")\n",
    "                    doc_schema = doc.get(\"schema\", \"\")\n",
    "                    doc_properties = doc.get(\"properties\", {})\n",
    "\n",
    "                    \n",
    "                    if doc_id not in node_types:\n",
    "                        doc_node_data = {\n",
    "                            \"id\": doc_id,\n",
    "                            \"schema\": doc_schema,\n",
    "                            \"type\": doc_schema,  \n",
    "                            \"created_at\": doc.get(\"created_at\", \"\"),\n",
    "                            \"updated_at\": doc.get(\"updated_at\", \"\"),\n",
    "                            \"fileSize\": doc_properties.get(\"fileSize\", [None])[0],\n",
    "                            \"fileName\": doc_properties.get(\"fileName\", [None])[0],\n",
    "                            \"mimeType\": doc_properties.get(\"mimeType\", [None])[0],\n",
    "                        }\n",
    "                        nodes.append(doc_node_data)\n",
    "                        node_types[doc_id] = doc_schema\n",
    "\n",
    "                    \n",
    "                    nested_body_text = doc_properties.get(\"bodyText\", [])\n",
    "                    if isinstance(nested_body_text, list) and nested_body_text:\n",
    "                        body_text_data.append({\n",
    "                            \"id\": doc_id,\n",
    "                            \"bodyText\": \" \".join(nested_body_text)\n",
    "                        })\n",
    "\n",
    "                    \n",
    "                    for rel_key, rel_values in doc_properties.items():\n",
    "                        if isinstance(rel_values, list):\n",
    "                            for rel_value in rel_values:\n",
    "                                if isinstance(rel_value, dict):\n",
    "                                    target_id = rel_value.get(\"id\", \"\")\n",
    "                                    target_type = rel_key  \n",
    "                                    if target_id and target_id not in node_types:\n",
    "                                        \n",
    "                                        target_node_data = {\n",
    "                                            \"id\": target_id,\n",
    "                                            \"schema\": doc_schema,\n",
    "                                            \"type\": target_type,\n",
    "                                        }\n",
    "                                        nodes.append(target_node_data)\n",
    "                                        node_types[target_id] = target_type\n",
    "\n",
    "                                    \n",
    "                                    relations.append({\n",
    "                                        \"source\": doc_id,\n",
    "                                        \"target\": target_id,\n",
    "                                        \"relationship\": f\"has {rel_key}\",\n",
    "                                        \"source_type\": node_types.get(doc_id, \"\"),\n",
    "                                        \"target_type\": node_types.get(target_id, \"\"),\n",
    "                                    })\n",
    "                                else:\n",
    "                                    target_id = str(rel_value)\n",
    "                                    target_type = rel_key  \n",
    "                                    if target_id not in node_types:\n",
    "                                        \n",
    "                                        target_node_data = {\n",
    "                                            \"id\": target_id,\n",
    "                                            \"schema\": doc_schema,\n",
    "                                            \"type\": target_type,\n",
    "                                        }\n",
    "                                        nodes.append(target_node_data)\n",
    "                                        node_types[target_id] = target_type\n",
    "\n",
    "                                    \n",
    "                                    relations.append({\n",
    "                                        \"source\": doc_id,\n",
    "                                        \"target\": target_id,\n",
    "                                        \"relationship\": f\"has {rel_key}\",\n",
    "                                        \"source_type\": node_types.get(doc_id, \"\"),\n",
    "                                        \"target_type\": node_types.get(target_id, \"\"),\n",
    "                                    })\n",
    "\n",
    "    nodes_df = pd.DataFrame(nodes).fillna(\"\")\n",
    "    relations_df = pd.DataFrame(relations).fillna(\"\")\n",
    "    body_text_df = pd.DataFrame(body_text_data).fillna(\"\")\n",
    "\n",
    "    if remove_orphans:\n",
    "        \n",
    "        connected_nodes = set(relations_df[\"source\"]).union(set(relations_df[\"target\"]))\n",
    "        nodes_df = nodes_df[nodes_df[\"id\"].isin(connected_nodes)]\n",
    "\n",
    "    return nodes_df, relations_df, body_text_df\n",
    "\n",
    "def save_body_text(body_text_df, output_file=\"body_text.csv\"):\n",
    "    \"\"\"Save bodyText data to a separate file.\"\"\"\n",
    "    body_text_df.to_csv(output_file, index=False)\n",
    "    print(f\"BodyText data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, relations, body_text = extract_metadata(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(nodes, relations, collection_id):\n",
    "    \"\"\"Save nodes and relations to CSV for Gephi.\"\"\"\n",
    "    nodes = nodes.fillna(\"\")\n",
    "    relations = relations.fillna(\"\")\n",
    "\n",
    "    nodes_file = f\"collection_{collection_id}_nodes.csv\"\n",
    "    relations_file = f\"collection_{collection_id}_relations.csv\"\n",
    "    nodes.to_csv(nodes_file, index=False)\n",
    "    relations.to_csv(relations_file, index=False)\n",
    "    print(f\"Nodes saved to {nodes_file}\")\n",
    "    print(f\"Relations saved to {relations_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adding specific filtering to ignore the processing information, language, etc that would link everything togetehr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(nodes_df, relations_df, output_file=\"graph.graphml\"):\n",
    "    \"\"\"Construct and save the graph in GraphML format.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, node in nodes_df.iterrows():\n",
    "        node_id = node[\"id\"]\n",
    "        node_attributes = node.drop(\"id\").to_dict()\n",
    "        G.add_node(node_id, **node_attributes)\n",
    "\n",
    "    for _, edge in relations_df.iterrows():\n",
    "        source = edge[\"source\"]\n",
    "        target = edge[\"target\"]\n",
    "        relationship = edge.get(\"relationship\", \"\")\n",
    "        G.add_edge(source, target, relationship=relationship)\n",
    "\n",
    "    nx.write_graphml(G, output_file)\n",
    "    print(f\"Graph saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to data/9_cleaned_graph.graphml\n"
     ]
    }
   ],
   "source": [
    "unwanted_types = [\n",
    "    'detectedLanguage',\n",
    "    'mimeType',\n",
    "    # 'ancestors', \n",
    "    'processingAgent',\n",
    "    'processingStatus',\n",
    "    'processedAt'\n",
    "]\n",
    "\n",
    "nodes[\"fileSize\"] = pd.to_numeric(nodes[\"fileSize\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "nodes_cleaned = nodes[~nodes['type'].isin(unwanted_types)]\n",
    "nodes_cleaned.to_csv(f\"data/{collection_id}_cleaned_nodes.csv\", index=False)\n",
    "\n",
    "relations_cleaned = relations[~relations['target_type'].isin(unwanted_types)]\n",
    "relations_cleaned.to_csv(f\"data/{collection_id}_cleaned_relations.csv\", index=False)\n",
    "\n",
    "save_graph(nodes_cleaned, relations_cleaned, output_file=f\"data/{collection_id}_cleaned_graph.graphml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BodyText data saved to data/9_body_text.csv\n"
     ]
    }
   ],
   "source": [
    "save_body_text(body_text, output_file=f\"data/{collection_id}_body_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Save all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graphml(relations, nodes, output_file=\"network.graphml\"):\n",
    "    for col in [\"source\", \"target\", \"relationship\", \"source_type\", \"target_type\"]:\n",
    "        if col not in relations.columns:\n",
    "            relations[col] = \"\"\n",
    "\n",
    "    relations[\"source\"] = relations[\"source\"].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "    relations[\"target\"] = relations[\"target\"].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, node in nodes.iterrows():\n",
    "        node_id = node[\"id\"]\n",
    "  \n",
    "        node_attributes = {\n",
    "            \"schema\": node.get(\"schema\", \"\"),  \n",
    "            \"fileName\": node.get(\"fileName\", \"\"),\n",
    "            \"fileSize\": int(node[\"fileSize\"]) if str(node.get(\"fileSize\", \"\")).isdigit() else 0,\n",
    "            \"mimeType\": node.get(\"mimeType\", \"\"),\n",
    "            \"language\": node.get(\"language\", \"\"),\n",
    "            \"created_at\": node.get(\"created_at\", \"\"),  \n",
    "            \"updated_at\": node.get(\"updated_at\", \"\"),  \n",
    "            \"contentHash\": node.get(\"contentHash\", \"\")\n",
    "        }\n",
    "  \n",
    "        G.add_node(node_id, **node_attributes)\n",
    "\n",
    "    \n",
    "    for _, row in relations.iterrows():\n",
    "        source = row[\"source\"]\n",
    "        target = row[\"target\"]\n",
    "        relationship = row[\"relationship\"]\n",
    "        source_type = row[\"source_type\"]\n",
    "        target_type = row[\"target_type\"]\n",
    "\n",
    "        G.add_edge(source, target, relationship=relationship, source_type=source_type, target_type=target_type)\n",
    "\n",
    "    try:\n",
    "        nx.write_graphml(G, output_file)\n",
    "        print(f\"GraphML saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save GraphML: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphML saved to collection_33.graphml\n"
     ]
    }
   ],
   "source": [
    "save_graphml(relations, nodes, output_file=f\"data/collection_{collection_id}.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes saved to collection_33_nodes.csv\n",
      "Relations saved to collection_33_relations.csv\n"
     ]
    }
   ],
   "source": [
    "save_csv(nodes, relations, collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BodyText data saved to body_text_33.csv\n"
     ]
    }
   ],
   "source": [
    "save_body_text(body_text, output_file=f\"data/body_text_{collection_id}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics extracted and saved to 'body_text_with_topics.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import pandas as pd\n",
    "\n",
    "body_texts = body_text['bodyText']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(body_texts)\n",
    "\n",
    "num_topics = 5  \n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "topic_matrix = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topics = []\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_keywords = [feature_names[i] for i in topic.argsort()[:-11:-1]] \n",
    "    topics.append(\" \".join(top_keywords)) \n",
    "\n",
    "document_topics = topic_matrix.argmax(axis=1) \n",
    "body_text['topic'] = [topics[topic_idx] for topic_idx in document_topics]\n",
    "\n",
    "body_text.to_csv(\"body_text_with_topics.csv\", index=False)\n",
    "print(\"Topics extracted and saved to 'body_text_with_topics.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
