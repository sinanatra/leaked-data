{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Structured Data from search.libraryofleaks.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giacomonanni/Documents/Development/leaked-data/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"API\"\n",
    "BASE_URL = \"https://search.libraryofleaks.org/api/2\"\n",
    "HEADERS = {\"Authorization\": f\"ApiKey {API_KEY}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_collections():\n",
    "    \"\"\"Fetch all available collections.\"\"\"\n",
    "    url = f\"{BASE_URL}/collections\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"results\"]\n",
    "\n",
    "def fetch_schemas(collection_id):\n",
    "    \"\"\"Fetch schemas available in a specific collection.\"\"\"\n",
    "    url = f\"{BASE_URL}/collections/{collection_id}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response.raise_for_status()\n",
    "    collection_data = response.json()\n",
    "    schema_values = collection_data.get(\"statistics\", {}).get(\"schema\", {}).get(\"values\", {})\n",
    "    return list(schema_values.keys())\n",
    "\n",
    "def fetch_all_entities_by_schemas(collection_id, limit=100, max_offset=9900):\n",
    "    \"\"\"\n",
    "    Fetch all entities across all schemas for a collection.\n",
    "    \"\"\"\n",
    "    all_entities = []\n",
    "    schemas = fetch_schemas(collection_id)\n",
    "\n",
    "    for schema in schemas:\n",
    "        print(f\"Fetching entities for schema: {schema}\")\n",
    "        schema_entities = fetch_entities_with_offset(collection_id, schema, limit=limit, max_offset=max_offset)\n",
    "        all_entities.extend(schema_entities)\n",
    "\n",
    "    return all_entities\n",
    "\n",
    "def fetch_entities_with_offset(collection_id, schema, limit=100, max_offset=9900, retries=3):\n",
    "    \"\"\"Fetch entities with automatic retries on rate limit errors.\"\"\"\n",
    "    entities = []\n",
    "    for offset in range(0, max_offset, limit):\n",
    "        params = {\n",
    "            \"collection_id\": collection_id,\n",
    "            \"schema\": schema,\n",
    "            \"filter:schemata\": schema,\n",
    "            \"limit\": limit,\n",
    "            \"offset\": offset,\n",
    "        }\n",
    "\n",
    "        url = f\"{BASE_URL}/entities\"\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            response = requests.get(url, headers=HEADERS, params=params)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit hit. Retrying in 30 seconds...\")\n",
    "                time.sleep(30)\n",
    "                attempt += 1\n",
    "                continue\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            entities.extend(data.get(\"results\", []))\n",
    "            print(f\"Fetched {len(data.get('results', []))} entities from offset {offset}\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed after {retries} retries for offset {offset}. Skipping this range.\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def fetch_entities_with_pagination(collection_id, schema, limit=100):\n",
    "    \"\"\"Fetch all entities for a specific collection and schema with pagination.\"\"\"\n",
    "    entities = []\n",
    "    params = {\"collection_id\": collection_id, \"schema\": schema, \"filter:schemata\": schema, \"limit\": limit}\n",
    "    url = f\"{BASE_URL}/entities\"\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url, headers=HEADERS, params=params if url == f\"{BASE_URL}/entities\" else None)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        entities.extend(data.get(\"results\", []))\n",
    "        url = data.get(\"next\")  \n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available collections:\n",
      "- 50: Aban Offshore\n",
      "- 13: Afghanistan Papiere\n",
      "- 51: Agencia Nacional de Hidrocarburos\n",
      "- 18: Airman Teixeira Leaks\n",
      "- 53: Alliance Coal\n",
      "- 35: Bahamas Registry\n",
      "- 1: BlueLeaks\n",
      "- 29: Chinga La Migra\n",
      "- 14: Constellis\n",
      "- 25: Cryptome Archive (2024)\n",
      "- 43: DJC Accountants\n",
      "- 15: Documents from US Espionage Den\n",
      "- 54: ENAMI EP\n",
      "- 39: Ethiopia Financial Intelligence Service\n",
      "- 19: FBIâ€™s Secret Rules\n",
      "- 56: Forest\n",
      "- 21: Fraternal Order of Police\n",
      "- 31: Fuck FBI Friday\n",
      "- 52: GorraLeaks\n",
      "- 55: Gulf Copper\n"
     ]
    }
   ],
   "source": [
    "collections = fetch_collections()\n",
    "print(\"Available collections:\")\n",
    "for col in collections:\n",
    "    print(f\"- {col['id']}: {col['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected:  33\n"
     ]
    }
   ],
   "source": [
    "collection_id = input(\"Enter the ID of the collection you want to explore: \")\n",
    "print(\"Selected: \", collection_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Page', 'HyperText', 'Image', 'Pages', 'Table', 'Person', 'PlainText', 'Workbook', 'Email', 'Folder', 'Document', 'Event', 'Package', 'Video', 'Audio']\n"
     ]
    }
   ],
   "source": [
    "schemas = fetch_schemas(collection_id)\n",
    "print(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching entities for schema: Page\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: HyperText\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Image\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Pages\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Table\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Person\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: PlainText\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Workbook\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Email\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Folder\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Document\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Event\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Package\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Video\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n",
      "Fetching entities for schema: Audio\n",
      "Fetched 100 entities from offset 0\n",
      "Fetched 100 entities from offset 100\n",
      "Fetched 100 entities from offset 200\n",
      "Fetched 100 entities from offset 300\n",
      "Fetched 100 entities from offset 400\n",
      "Fetched 100 entities from offset 500\n",
      "Fetched 100 entities from offset 600\n",
      "Fetched 100 entities from offset 700\n",
      "Fetched 100 entities from offset 800\n",
      "Fetched 100 entities from offset 900\n"
     ]
    }
   ],
   "source": [
    "all_entities = fetch_all_entities_by_schemas(collection_id, max_offset=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entities fetched: 15000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total entities fetched: {len(all_entities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(entities, remove_orphans=True):\n",
    "    \"\"\"Extract nodes and relationships from entities, handling orphan nodes.\"\"\"\n",
    "    nodes = []\n",
    "    relations = []\n",
    "    node_ids = set() \n",
    "\n",
    "    for entity in entities:\n",
    "        source = entity.get(\"id\", \"\")\n",
    "        schema = entity.get(\"schema\", \"\")\n",
    "        properties = entity.get(\"properties\", {})\n",
    "\n",
    "        if source not in node_ids:\n",
    "            node_data = {\n",
    "                \"id\": source,\n",
    "                \"schema\": schema,\n",
    "                \"type\": schema,  \n",
    "                \"created_at\": entity.get(\"created_at\", \"\"),\n",
    "                \"updated_at\": entity.get(\"updated_at\", \"\"),\n",
    "            }\n",
    "            nodes.append(node_data)\n",
    "            node_ids.add(source)\n",
    "\n",
    "        \n",
    "        documents = properties.get(\"document\", [])\n",
    "        if isinstance(documents, list):\n",
    "            for doc in documents:\n",
    "                if isinstance(doc, dict):  \n",
    "                    doc_id = doc.get(\"id\", \"\")\n",
    "                    doc_schema = doc.get(\"schema\", \"\")\n",
    "                    doc_properties = doc.get(\"properties\", {})\n",
    "\n",
    "                    \n",
    "                    if doc_id not in node_ids:\n",
    "                        doc_node_data = {\n",
    "                            \"id\": doc_id,\n",
    "                            \"schema\": doc_schema,\n",
    "                            \"type\": doc_schema,  \n",
    "                            \"created_at\": doc.get(\"created_at\", \"\"),\n",
    "                            \"updated_at\": doc.get(\"updated_at\", \"\"),\n",
    "                        }\n",
    "                        nodes.append(doc_node_data)\n",
    "                        node_ids.add(doc_id)\n",
    "\n",
    "                    \n",
    "                    for rel_key, rel_values in doc_properties.items():\n",
    "                        if isinstance(rel_values, list):\n",
    "                            for rel_value in rel_values:\n",
    "                                if isinstance(rel_value, dict):\n",
    "                                    target_id = rel_value.get(\"id\", \"\")\n",
    "                                    if target_id and target_id not in node_ids:\n",
    "                                        \n",
    "                                        target_node_data = {\n",
    "                                            \"id\": target_id,\n",
    "                                            \"schema\": rel_key,  \n",
    "                                            \"type\": rel_key,  \n",
    "                                        }\n",
    "                                        nodes.append(target_node_data)\n",
    "                                        node_ids.add(target_id)\n",
    "\n",
    "                                    \n",
    "                                    relations.append({\n",
    "                                        \"source\": doc_id,\n",
    "                                        \"target\": target_id,\n",
    "                                        \"relationship\": f\"has {rel_key}\",\n",
    "                                    })\n",
    "                                else:\n",
    "                                    target_id = str(rel_value)\n",
    "                                    if target_id not in node_ids:\n",
    "                                        \n",
    "                                        target_node_data = {\n",
    "                                            \"id\": target_id,\n",
    "                                            \"schema\": rel_key,  \n",
    "                                            \"type\": rel_key,  \n",
    "                                        }\n",
    "                                        nodes.append(target_node_data)\n",
    "                                        node_ids.add(target_id)\n",
    "\n",
    "                                    \n",
    "                                    relations.append({\n",
    "                                        \"source\": doc_id,\n",
    "                                        \"target\": target_id,\n",
    "                                        \"relationship\": f\"has {rel_key}\",\n",
    "                                    })\n",
    "\n",
    "    nodes_df = pd.DataFrame(nodes).fillna(\"\")\n",
    "    relations_df = pd.DataFrame(relations).fillna(\"\")\n",
    "\n",
    "    if remove_orphans:\n",
    "        connected_nodes = set(relations_df[\"source\"]).union(set(relations_df[\"target\"]))\n",
    "        nodes_df = nodes_df[nodes_df[\"id\"].isin(connected_nodes)]\n",
    "\n",
    "    return nodes_df, relations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, relations = extract_metadata(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(nodes, relations, collection_id):\n",
    "    \"\"\"Save nodes and relations to CSV for Gephi.\"\"\"\n",
    "    nodes = nodes.fillna(\"\")\n",
    "    relations = relations.fillna(\"\")\n",
    "\n",
    "    nodes_file = f\"collection_{collection_id}_nodes.csv\"\n",
    "    relations_file = f\"collection_{collection_id}_relations.csv\"\n",
    "    nodes.to_csv(nodes_file, index=False)\n",
    "    relations.to_csv(relations_file, index=False)\n",
    "    print(f\"Nodes saved to {nodes_file}\")\n",
    "    print(f\"Relations saved to {relations_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graphml(relations, nodes, output_file=\"network.graphml\"):\n",
    "    for col in [\"source\", \"target\", \"relationship\", \"source_type\", \"target_type\"]:\n",
    "        if col not in relations.columns:\n",
    "            relations[col] = \"\"\n",
    "\n",
    "    relations[\"source\"] = relations[\"source\"].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "    relations[\"target\"] = relations[\"target\"].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, node in nodes.iterrows():\n",
    "        node_id = node[\"id\"]\n",
    "  \n",
    "        node_attributes = {\n",
    "            \"schema\": node.get(\"schema\", \"\"),  \n",
    "            \"fileName\": node.get(\"fileName\", \"\"),\n",
    "            \"fileSize\": int(node[\"fileSize\"]) if str(node.get(\"fileSize\", \"\")).isdigit() else 0,\n",
    "            \"mimeType\": node.get(\"mimeType\", \"\"),\n",
    "            \"language\": node.get(\"language\", \"\"),\n",
    "            \"created_at\": node.get(\"created_at\", \"\"),  \n",
    "            \"updated_at\": node.get(\"updated_at\", \"\"),  \n",
    "            \"contentHash\": node.get(\"contentHash\", \"\")\n",
    "        }\n",
    "  \n",
    "        G.add_node(node_id, **node_attributes)\n",
    "\n",
    "    \n",
    "    for _, row in relations.iterrows():\n",
    "        source = row[\"source\"]\n",
    "        target = row[\"target\"]\n",
    "        relationship = row[\"relationship\"]\n",
    "        source_type = row[\"source_type\"]\n",
    "        target_type = row[\"target_type\"]\n",
    "\n",
    "        G.add_edge(source, target, relationship=relationship, source_type=source_type, target_type=target_type)\n",
    "\n",
    "    try:\n",
    "        nx.write_graphml(G, output_file)\n",
    "        print(f\"GraphML saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save GraphML: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphML saved to collection_33.graphml\n"
     ]
    }
   ],
   "source": [
    "save_graphml(relations, nodes, output_file=f\"collection_{collection_id}.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes saved to collection_33_nodes.csv\n",
      "Relations saved to collection_33_relations.csv\n"
     ]
    }
   ],
   "source": [
    "save_csv(nodes, relations, collection_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
